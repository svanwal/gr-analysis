{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82204626",
   "metadata": {},
   "source": [
    "## Importing modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a32d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svanwal/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Importing packages\n",
    "import osmnx  as ox\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import folium\n",
    "import os.path\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Importing modules\n",
    "import gr_mapmatch # Contains functions that perform the map matching of roads\n",
    "import gr_placematch # Contains functions that perform the map matching of places\n",
    "import gr_utils # Contains useful geometry functions\n",
    "import gr_plot\n",
    "\n",
    "# Configuring modules & packages\n",
    "ox.settings.useful_tags_way = [\n",
    "    \"bridge\",\"tunnel\",\"name\",\"highway\",\"area\",\"landuse\",\"surface\",\"tracktype\"\n",
    "] # Configuring which parameters we want to obtain from OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b93e47",
   "metadata": {},
   "source": [
    "## Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015a24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trailname = 'gr16' # Name of the hiking trail to be considered (will search for trail.csv or trail.gpx as sources)\n",
    "delta = 0.005 # Tolerance around bounding box per trail section [deg]\n",
    "points_per_batch = 100 # Subdivide the trail into batches of this many points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e8e6f",
   "metadata": {},
   "source": [
    "## Loading GPX file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667a640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trail points from <data_input/gr16.gpx>...\n",
      "Finished loading.\n"
     ]
    }
   ],
   "source": [
    "filename_gpx = 'data_input/' + trailname + '.gpx'\n",
    "filename_csv = 'data_output/' + trailname + '.csv'\n",
    "if not os.path.isfile(filename_csv): # The GPX file was not processed into a clean CSV file before\n",
    "    if not os.path.isfile(filename_gpx): # The GPX file does not exist, throw error\n",
    "        raise ValueError(f'The GPX file <{filename_gpx}> was not found! Please make sure it exists.')\n",
    "    else: # The GPX file exists, so convert it into a clean CSV file\n",
    "        print(f'Converting GPX file <{filename_gpx}> into cleaned CSV file <{filename_csv}>...')\n",
    "        gr_utils.process_gpx(filename_gpx,filename_csv)\n",
    "        print('Completed conversion.')\n",
    "print(f'Loading trail points from <{filename_gpx}>...')\n",
    "trail = pd.read_csv(filename_csv) # Now read the cleaned CSV file into a DataFrame (latitude, longitude, elevation)\n",
    "print('Finished loading.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd817d",
   "metadata": {},
   "source": [
    "## Gathering road information from OSM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1bdd1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling 0 of 60 that covers GPX track points 0 through 100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 1 of 60 that covers GPX track points 100 through 200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 2 of 60 that covers GPX track points 200 through 300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 3 of 60 that covers GPX track points 300 through 400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 4 of 60 that covers GPX track points 400 through 500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 5 of 60 that covers GPX track points 500 through 600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 6 of 60 that covers GPX track points 600 through 700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 7 of 60 that covers GPX track points 700 through 800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 8 of 60 that covers GPX track points 800 through 900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 9 of 60 that covers GPX track points 900 through 1000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 10 of 60 that covers GPX track points 1000 through 1100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 11 of 60 that covers GPX track points 1100 through 1200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 12 of 60 that covers GPX track points 1200 through 1300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 13 of 60 that covers GPX track points 1300 through 1400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 14 of 60 that covers GPX track points 1400 through 1500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 15 of 60 that covers GPX track points 1500 through 1600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 16 of 60 that covers GPX track points 1600 through 1700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 17 of 60 that covers GPX track points 1700 through 1800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 18 of 60 that covers GPX track points 1800 through 1900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 19 of 60 that covers GPX track points 1900 through 2000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 20 of 60 that covers GPX track points 2000 through 2100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 21 of 60 that covers GPX track points 2100 through 2200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 22 of 60 that covers GPX track points 2200 through 2300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 23 of 60 that covers GPX track points 2300 through 2400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 24 of 60 that covers GPX track points 2400 through 2500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 25 of 60 that covers GPX track points 2500 through 2600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 26 of 60 that covers GPX track points 2600 through 2700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 27 of 60 that covers GPX track points 2700 through 2800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 28 of 60 that covers GPX track points 2800 through 2900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 29 of 60 that covers GPX track points 2900 through 3000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 30 of 60 that covers GPX track points 3000 through 3100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 31 of 60 that covers GPX track points 3100 through 3200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 32 of 60 that covers GPX track points 3200 through 3300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 33 of 60 that covers GPX track points 3300 through 3400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 34 of 60 that covers GPX track points 3400 through 3500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 35 of 60 that covers GPX track points 3500 through 3600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 36 of 60 that covers GPX track points 3600 through 3700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 37 of 60 that covers GPX track points 3700 through 3800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 38 of 60 that covers GPX track points 3800 through 3900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 39 of 60 that covers GPX track points 3900 through 4000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 40 of 60 that covers GPX track points 4000 through 4100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 41 of 60 that covers GPX track points 4100 through 4200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 42 of 60 that covers GPX track points 4200 through 4300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 43 of 60 that covers GPX track points 4300 through 4400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 44 of 60 that covers GPX track points 4400 through 4500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 45 of 60 that covers GPX track points 4500 through 4600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 46 of 60 that covers GPX track points 4600 through 4700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 47 of 60 that covers GPX track points 4700 through 4800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 48 of 60 that covers GPX track points 4800 through 4900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 49 of 60 that covers GPX track points 4900 through 5000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 50 of 60 that covers GPX track points 5000 through 5100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 51 of 60 that covers GPX track points 5100 through 5200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 52 of 60 that covers GPX track points 5200 through 5300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 53 of 60 that covers GPX track points 5300 through 5400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 54 of 60 that covers GPX track points 5400 through 5500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 55 of 60 that covers GPX track points 5500 through 5600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 56 of 60 that covers GPX track points 5600 through 5700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 57 of 60 that covers GPX track points 5700 through 5800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 58 of 60 that covers GPX track points 5800 through 5900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 59 of 60 that covers GPX track points 5900 through 6000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 60 of 60 that covers GPX track points 6000 through 6052...\n",
      "   This batch was processed before, skipping.\n"
     ]
    }
   ],
   "source": [
    "# TODO - make this optional only if the roads file is not found!!!\n",
    "\n",
    "# Matching GPX track to OSM network (uses _osm_network_download under the hood)\n",
    "n_trail = len(trail) # Number of GPX points in the trail\n",
    "n_batch = int(np.ceil(trail.shape[0]/points_per_batch)) # Number of batches to be run\n",
    "for b in range(n_batch): # Using batch counter b\n",
    "    \n",
    "    # Define the range of GPX points to process in the current batch\n",
    "    n1 = b*points_per_batch # First point of this batch\n",
    "    n2 = min(n1 + points_per_batch, n_trail) # Last point of this batch (clipped)\n",
    "    trail_section = trail.loc[n1:n2] # Select that range of GPX points\n",
    "    trail_coords  = gr_mapmatch.trail_to_coords(trail_section) # Convert the points into a list of [lat, lon] pairs\n",
    "    \n",
    "    # Check if this batch was processed before\n",
    "    # TODO - update name to have _roads_ in there\n",
    "    batch_out = f'cache/{trailname}_{n1}to{n2}.csv'\n",
    "    print(f'Handling {b} of {n_batch-1} that covers GPX track points {n1} through {n2}...')\n",
    "    if os.path.isfile(batch_out): # It already exists\n",
    "        print('   This batch was processed before, skipping.')\n",
    "    else: # It does not exist, so process it\n",
    "        network, segment_list = gr_mapmatch.match_batch(trail_section, trail_coords, delta)\n",
    "        gr_utils.write_batch(batch_out, segment_list)\n",
    "        print('   Finished this batch.')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca7e50b",
   "metadata": {},
   "source": [
    "## Merging road information & removing backtracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7157cd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged section file...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "filename_roads = 'cache/' + trailname + '_roads.csv'\n",
    "if not os.path.isfile(filename_roads): # The merged file does not exist\n",
    "    print('Merged section file was not found, merging and saving...')\n",
    "    data_roads_raw = gr_utils.merge_roads(trailname, trail, points_per_batch) # Merge the different sections\n",
    "    data_roads = gr_mapmatch.remove_repeat_segments(data_roads_raw) # Remove backtracked sections\n",
    "    gr_utils.write_roads(trailname, data_roads) # Write the merged sections\n",
    "    print('Saved.')\n",
    "else: # The merged file does exist\n",
    "    print('Loading merged section file...')\n",
    "    data_roads = gr_utils.read_roads(trailname) # Read the merged sections\n",
    "    print('Loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289f2ec",
   "metadata": {},
   "source": [
    "## Gathering place information from OSM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f118105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling 0 of 62 that covers road segments 0 through 99...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 1 of 62 that covers road segments 100 through 199...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 2 of 62 that covers road segments 200 through 299...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 3 of 62 that covers road segments 300 through 399...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 4 of 62 that covers road segments 400 through 499...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 5 of 62 that covers road segments 500 through 599...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 6 of 62 that covers road segments 600 through 699...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 7 of 62 that covers road segments 700 through 799...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 8 of 62 that covers road segments 800 through 899...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 9 of 62 that covers road segments 900 through 999...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 10 of 62 that covers road segments 1000 through 1099...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 11 of 62 that covers road segments 1100 through 1199...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 12 of 62 that covers road segments 1200 through 1299...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 13 of 62 that covers road segments 1300 through 1399...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 14 of 62 that covers road segments 1400 through 1499...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 15 of 62 that covers road segments 1500 through 1599...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 16 of 62 that covers road segments 1600 through 1699...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 17 of 62 that covers road segments 1700 through 1799...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 18 of 62 that covers road segments 1800 through 1899...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 19 of 62 that covers road segments 1900 through 1999...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 20 of 62 that covers road segments 2000 through 2099...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 21 of 62 that covers road segments 2100 through 2199...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 22 of 62 that covers road segments 2200 through 2299...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 23 of 62 that covers road segments 2300 through 2399...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 24 of 62 that covers road segments 2400 through 2499...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 25 of 62 that covers road segments 2500 through 2599...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 26 of 62 that covers road segments 2600 through 2699...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 27 of 62 that covers road segments 2700 through 2799...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 28 of 62 that covers road segments 2800 through 2899...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 29 of 62 that covers road segments 2900 through 2999...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 30 of 62 that covers road segments 3000 through 3099...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 31 of 62 that covers road segments 3100 through 3199...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 32 of 62 that covers road segments 3200 through 3299...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 33 of 62 that covers road segments 3300 through 3399...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 34 of 62 that covers road segments 3400 through 3499...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 35 of 62 that covers road segments 3500 through 3599...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 36 of 62 that covers road segments 3600 through 3699...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 37 of 62 that covers road segments 3700 through 3799...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 38 of 62 that covers road segments 3800 through 3899...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 39 of 62 that covers road segments 3900 through 3999...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 40 of 62 that covers road segments 4000 through 4099...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 41 of 62 that covers road segments 4100 through 4199...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 42 of 62 that covers road segments 4200 through 4299...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 43 of 62 that covers road segments 4300 through 4399...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 44 of 62 that covers road segments 4400 through 4499...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 45 of 62 that covers road segments 4500 through 4599...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 46 of 62 that covers road segments 4600 through 4699...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 47 of 62 that covers road segments 4700 through 4799...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 48 of 62 that covers road segments 4800 through 4899...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 49 of 62 that covers road segments 4900 through 4999...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 50 of 62 that covers road segments 5000 through 5099...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 51 of 62 that covers road segments 5100 through 5199...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 52 of 62 that covers road segments 5200 through 5299...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 53 of 62 that covers road segments 5300 through 5399...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 54 of 62 that covers road segments 5400 through 5499...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 55 of 62 that covers road segments 5500 through 5599...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 56 of 62 that covers road segments 5600 through 5699...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 57 of 62 that covers road segments 5700 through 5799...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 58 of 62 that covers road segments 5800 through 5899...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 59 of 62 that covers road segments 5900 through 5999...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 60 of 62 that covers road segments 6000 through 6099...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 61 of 62 that covers road segments 6100 through 6199...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 62 of 62 that covers road segments 6200 through 6257...\n",
      "   This batch was processed before, skipping.\n"
     ]
    }
   ],
   "source": [
    "## Matching GPX track to OSM places (uses _osm_place_download under the hood)\n",
    "n_roads = len(data_roads) # Number of segments in data_roads\n",
    "points_per_batch_places = 100 # Subdivide the trail into batches of this many segments\n",
    "n_batch_places = int(np.ceil(n_roads/points_per_batch_places)) # Number of batches to be run\n",
    "delta_places = 0.005 # bbox delta in deg\n",
    "data_roads['dev_dist'] = 0.0 # filling\n",
    "for b in range(n_batch_places): # Using batch counter b\n",
    "    \n",
    "    # Define the range of segments to process in the current batch\n",
    "    n1 = b*points_per_batch_places # First point of this batch\n",
    "    n2 = min(n1 + points_per_batch_places, n_roads) - 1 # Last point of this batch (clipped)\n",
    "\n",
    "    # Check if this batch was processed before\n",
    "    batch_out = f'cache/{trailname}_places_{n1}to{n2}.csv'\n",
    "    print(f'Handling {b} of {n_batch_places-1} that covers road segments {n1} through {n2}...')\n",
    "    if os.path.isfile(batch_out): # It already exists\n",
    "        print('   This batch was processed before, skipping.')\n",
    "    else: # It does not exist, so process it (use loc to avoid selection error)\n",
    "        data_roads.loc[n1:n2,'dev_dist'] = gr_placematch.match_batch(data_roads.loc[n1:n2], delta_places)\n",
    "        gr_utils.write_batch_places(batch_out, data_roads.loc[n1:n2])\n",
    "        print('   Finished this batch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3277b065",
   "metadata": {},
   "source": [
    "## Merging place information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da54f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged section file...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "filename_places = 'cache/' + trailname + '_places.csv'\n",
    "if not os.path.isfile(filename_places): # The merged file does not exist\n",
    "    print('Merged places file was not found, merging...')\n",
    "    data_places = gr_utils.merge_places(trailname, data_roads, points_per_batch_places) # Merge the different sections\n",
    "    print('Saving...')\n",
    "    gr_utils.write_places(trailname, data_places) # Write the merged sections\n",
    "    print('Saved.')\n",
    "else: # The merged file does exist\n",
    "    print('Loading merged section file...')\n",
    "    data_places = gr_utils.read_places(trailname) # Read the merged sections\n",
    "    print('Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324df6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5913      2.042885\n",
       "5914      2.103423\n",
       "5915      2.192480\n",
       "5916      2.260855\n",
       "5917      2.285231\n",
       "           ...    \n",
       "6008    999.900000\n",
       "6009    999.900000\n",
       "6010    999.900000\n",
       "6011    999.900000\n",
       "6012    999.900000\n",
       "Name: dev_dist, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_places['dev_dist'].tail(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8144146",
   "metadata": {},
   "source": [
    "## Plotting places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ffa18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_d = 1.0 # Consider a segment developed if it lies closer than tol_d to a developed area\n",
    "filepath = gr_plot.show_development(data_places,tol_d)\n",
    "IFrame(filepath, width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312b617",
   "metadata": {},
   "source": [
    "## Establishing paved, traffic, and development type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482acdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e644077",
   "metadata": {},
   "source": [
    "## Saving completed data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9c24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e7f5d87",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c488ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911ad21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382aa1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27707a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f5325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9ce11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44435420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca63a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d5ad7db",
   "metadata": {},
   "source": [
    "## Drafts below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_polygon(row):\n",
    "#     is_simplepoly = type(row['geometry']) is shapely.geometry.polygon.Polygon\n",
    "#     is_multipoly = type(row['geometry']) is shapely.geometry.multipolygon.MultiPolygon\n",
    "#     return is_simplepoly or is_multipoly\n",
    "# #     return type(row['geometry']) is shapely.geometry.polygon.Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087224e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Matching GPX track to OSM places (uses _osm_place_download under the hood)\n",
    "# points_per_batch_places = 100 # Subdivide the trail into batches of this many segments\n",
    "# # n1 = 0;\n",
    "# # n2 = points_per_batch_places;\n",
    "# n1 = 0*points_per_batch_places;\n",
    "# n2 = 5*points_per_batch_places;\n",
    "# delta = 0.005\n",
    "\n",
    "# # Select point subset & construct bbox\n",
    "# subset = data_roads.iloc[n1:n2] # subset of points dataframe\n",
    "# lat_min = subset['x0'].min() - delta\n",
    "# lat_max = subset['x0'].max() + delta\n",
    "# lon_min = subset['y1'].min() - delta\n",
    "# lon_max = subset['y1'].max() + delta\n",
    "# polygon = ox.utils_geo.bbox_to_poly(lat_max, lat_min, lon_max, lon_min) # polygon of bbox around subset\n",
    "\n",
    "# # Download place data with landuse tag\n",
    "# # tags = {\"landuse\": True}\n",
    "# tags = {\"landuse\": ['commercial','construction','education','industrial','residential','retail','institutional','farmyard','cemetery','garages','railway','landfill','brownfield','quarry']}\n",
    "# gdf = ox.geometries_from_polygon(polygon, tags)\n",
    "# gdf['is_polygon'] = gdf.apply(is_polygon, axis=1)\n",
    "# mask_polygon = gdf['is_polygon']==True\n",
    "# gdf_subset = gdf[mask_polygon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test matching\n",
    "# developed = []\n",
    "# tol_d = 1.0 # Consider a segment developed if it lies closer than tol_d to a developed area\n",
    "# for i, segment in subset.iterrows():\n",
    "#     developed.append(False)\n",
    "#     xmid = (segment['x0'] + segment['x1'])/2\n",
    "#     ymid = (segment['y0'] + segment['y1'])/2\n",
    "#     point = shapely.geometry.Point(ymid,xmid)\n",
    "#     for j, area in gdf_subset.iterrows():\n",
    "#         d = 1000*area['geometry'].distance(point)\n",
    "#         if d<tol_d:\n",
    "#             developed[i] = True\n",
    "#             break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b52ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a map of the subset & polygons near it\n",
    "# # Map setup\n",
    "# chart = folium.Map(location=trail_coords[0], zoom_start=12, tiles=\"OpenStreetMap\")\n",
    "\n",
    "# # Draw map matched frame\n",
    "# xy = gr_plot.get_coords_from_frame(subset)\n",
    "\n",
    "# # Draw areas\n",
    "# for i in range(gdf_subset.shape[0]):\n",
    "#     item = gdf_subset.iloc[i]\n",
    "#     sim_geo = gpd.GeoSeries(item['geometry'])\n",
    "#     geo_j = sim_geo.to_json()\n",
    "#     geo_j = folium.GeoJson(data=geo_j,style_function=lambda x: {'fillColor': 'orange'})\n",
    "#     folium.Popup(item['landuse']).add_to(geo_j)\n",
    "#     geo_j.add_to(chart)\n",
    "    \n",
    "# for point in xy:\n",
    "#     newmarker = folium.CircleMarker(location=point,radius=1,color='black')\n",
    "#     newmarker.add_to(chart)\n",
    "\n",
    "# # Draw midpoints according to development status\n",
    "# for idx, row in subset.iterrows():\n",
    "#     x0 = row['x0']\n",
    "#     x1 = row['x1']\n",
    "#     y0 = row['y0']\n",
    "#     y1 = row['y1']\n",
    "#     xmid = (x0 + x1)/2\n",
    "#     ymid = (y0 + y1)/2\n",
    "#     midpoint = [xmid,ymid]\n",
    "# #     newmarker = folium.CircleMarker(location=point,radius=2,color='black')\n",
    "# #     newmarker.add_to(chart)\n",
    "#     if developed[idx]:\n",
    "#         newmarker = folium.CircleMarker(location=midpoint,radius=2,color='red')\n",
    "#     else:\n",
    "#         newmarker = folium.CircleMarker(location=midpoint,radius=2,color='green')\n",
    "#     newmarker.add_to(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2601c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a plot of the original and matched track\n",
    "# trail_coords  = gr_mapmatch.trail_to_coords(trail)\n",
    "# filepath = gr_plot.compare_tracks(trail_coords, data_roads)\n",
    "# IFrame(filepath, width=1000, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Render the map\n",
    "# filepath = \"cache/chart_development.html\"\n",
    "# chart.save(filepath)\n",
    "# IFrame(filepath, width=1000, height=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
