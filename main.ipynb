{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82204626",
   "metadata": {},
   "source": [
    "## Importing modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f4a32d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import osmnx  as ox\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import folium\n",
    "import os.path\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Importing modules\n",
    "import gr_mapmatch # Contains functions that perform the map matching\n",
    "import gr_utils # Contains useful geometry functions\n",
    "import gr_plot\n",
    "\n",
    "# Configuring modules & packages\n",
    "ox.settings.useful_tags_way = [\n",
    "    \"bridge\",\"tunnel\",\"name\",\"highway\",\"area\",\"landuse\",\"surface\",\"tracktype\"\n",
    "] # Configuring which parameters we want to obtain from OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b93e47",
   "metadata": {},
   "source": [
    "## Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015a24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trailname = 'gr16' # Name of the hiking trail to be considered (will search for trail.csv or trail.gpx as sources)\n",
    "delta = 0.005 # Tolerance around bounding box per trail section [deg]\n",
    "points_per_batch = 100 # Subdivide the trail into batches of this many points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e8e6f",
   "metadata": {},
   "source": [
    "## Loading GPX file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667a640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trail points from <data_input/gr16.gpx>...\n",
      "Finished loading.\n"
     ]
    }
   ],
   "source": [
    "filename_gpx = 'data_input/' + trailname + '.gpx'\n",
    "filename_csv = 'data_output/' + trailname + '.csv'\n",
    "if not os.path.isfile(filename_csv): # The GPX file was not processed into a clean CSV file before\n",
    "    if not os.path.isfile(filename_gpx): # The GPX file does not exist, throw error\n",
    "        raise ValueError(f'The GPX file <{filename_gpx}> was not found! Please make sure it exists.')\n",
    "    else: # The GPX file exists, so convert it into a clean CSV file\n",
    "        print(f'Converting GPX file <{filename_gpx}> into cleaned CSV file <{filename_csv}>...')\n",
    "        gr_utils.process_gpx(filename_gpx,filename_csv)\n",
    "        print('Completed conversion.')\n",
    "print(f'Loading trail points from <{filename_gpx}>...')\n",
    "trail = pd.read_csv(filename_csv) # Now read the cleaned CSV file into a DataFrame (latitude, longitude, elevation)\n",
    "print('Finished loading.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd817d",
   "metadata": {},
   "source": [
    "## Matching GPX track to OSM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1bdd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling 0 of 60 that covers GPX track points 0 through 100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 1 of 60 that covers GPX track points 100 through 200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 2 of 60 that covers GPX track points 200 through 300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 3 of 60 that covers GPX track points 300 through 400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 4 of 60 that covers GPX track points 400 through 500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 5 of 60 that covers GPX track points 500 through 600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 6 of 60 that covers GPX track points 600 through 700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 7 of 60 that covers GPX track points 700 through 800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 8 of 60 that covers GPX track points 800 through 900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 9 of 60 that covers GPX track points 900 through 1000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 10 of 60 that covers GPX track points 1000 through 1100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 11 of 60 that covers GPX track points 1100 through 1200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 12 of 60 that covers GPX track points 1200 through 1300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 13 of 60 that covers GPX track points 1300 through 1400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 14 of 60 that covers GPX track points 1400 through 1500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 15 of 60 that covers GPX track points 1500 through 1600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 16 of 60 that covers GPX track points 1600 through 1700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 17 of 60 that covers GPX track points 1700 through 1800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 18 of 60 that covers GPX track points 1800 through 1900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 19 of 60 that covers GPX track points 1900 through 2000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 20 of 60 that covers GPX track points 2000 through 2100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 21 of 60 that covers GPX track points 2100 through 2200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 22 of 60 that covers GPX track points 2200 through 2300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 23 of 60 that covers GPX track points 2300 through 2400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 24 of 60 that covers GPX track points 2400 through 2500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 25 of 60 that covers GPX track points 2500 through 2600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 26 of 60 that covers GPX track points 2600 through 2700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 27 of 60 that covers GPX track points 2700 through 2800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 28 of 60 that covers GPX track points 2800 through 2900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 29 of 60 that covers GPX track points 2900 through 3000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 30 of 60 that covers GPX track points 3000 through 3100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 31 of 60 that covers GPX track points 3100 through 3200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 32 of 60 that covers GPX track points 3200 through 3300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 33 of 60 that covers GPX track points 3300 through 3400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 34 of 60 that covers GPX track points 3400 through 3500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 35 of 60 that covers GPX track points 3500 through 3600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 36 of 60 that covers GPX track points 3600 through 3700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 37 of 60 that covers GPX track points 3700 through 3800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 38 of 60 that covers GPX track points 3800 through 3900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 39 of 60 that covers GPX track points 3900 through 4000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 40 of 60 that covers GPX track points 4000 through 4100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 41 of 60 that covers GPX track points 4100 through 4200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 42 of 60 that covers GPX track points 4200 through 4300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 43 of 60 that covers GPX track points 4300 through 4400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 44 of 60 that covers GPX track points 4400 through 4500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 45 of 60 that covers GPX track points 4500 through 4600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 46 of 60 that covers GPX track points 4600 through 4700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 47 of 60 that covers GPX track points 4700 through 4800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 48 of 60 that covers GPX track points 4800 through 4900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 49 of 60 that covers GPX track points 4900 through 5000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 50 of 60 that covers GPX track points 5000 through 5100...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 51 of 60 that covers GPX track points 5100 through 5200...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 52 of 60 that covers GPX track points 5200 through 5300...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 53 of 60 that covers GPX track points 5300 through 5400...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 54 of 60 that covers GPX track points 5400 through 5500...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 55 of 60 that covers GPX track points 5500 through 5600...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 56 of 60 that covers GPX track points 5600 through 5700...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 57 of 60 that covers GPX track points 5700 through 5800...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 58 of 60 that covers GPX track points 5800 through 5900...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 59 of 60 that covers GPX track points 5900 through 6000...\n",
      "   This batch was processed before, skipping.\n",
      "Handling 60 of 60 that covers GPX track points 6000 through 6052...\n",
      "   This batch was processed before, skipping.\n"
     ]
    }
   ],
   "source": [
    "# Matching GPX track to OSM network (uses _osm_network_download under the hood)\n",
    "n_trail = len(trail) # Number of GPX points in the trail\n",
    "n_batch = int(np.ceil(trail.shape[0]/points_per_batch)) # Number of batches to be run\n",
    "for b in range(n_batch): # Using batch counter b\n",
    "    \n",
    "    # Define the range of GPX points to process in the current batch\n",
    "    n1 = b*points_per_batch # First point of this batch\n",
    "    n2 = min(n1 + points_per_batch, n_trail) # Last point of this batch (clipped)\n",
    "    trail_section = trail.loc[n1:n2] # Select that range of GPX points\n",
    "    trail_coords  = gr_mapmatch.trail_to_coords(trail_section) # Convert the points into a list of [lat, lon] pairs\n",
    "    \n",
    "    # Check if this batch was processed before\n",
    "    batch_out = f'cache/{trailname}_{n1}to{n2}.csv'\n",
    "    print(f'Handling {b} of {n_batch-1} that covers GPX track points {n1} through {n2}...')\n",
    "    if os.path.isfile(batch_out): # It already exists\n",
    "        print('   This batch was processed before, skipping.')\n",
    "    else: # It does not exist, so process it\n",
    "        network, segment_list = gr_mapmatch.match_batch(trail_section, trail_coords, delta)\n",
    "        gr_utils.write_batch(batch_out, segment_list)\n",
    "        print('   Finished this batch.')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca7e50b",
   "metadata": {},
   "source": [
    "## Merging sections and removing backtracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7157cd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged section file...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "filename_roads = 'cache/' + trailname + '_roads.csv'\n",
    "if not os.path.isfile(filename_roads): # The merged file does not exist\n",
    "    print('Merged section file was not found, merging and saving...')\n",
    "    data_roads_raw = gr_utils.merge_roads(trailname, trail, points_per_batch) # Merge the different sections\n",
    "    data_roads = gr_mapmatch.remove_repeat_segments(data_roads_raw) # Remove backtracked sections\n",
    "    gr_utils.write_roads(trailname, data_roads) # Write the merged sections\n",
    "    print('Saved.')\n",
    "else: # The merged file does exist\n",
    "    print('Loading merged section file...')\n",
    "    data_roads = gr_utils.read_roads(trailname) # Read the merged sections\n",
    "    print('Loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289f2ec",
   "metadata": {},
   "source": [
    "## Gathering place information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b39aadcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_polygon(row):\n",
    "    is_simplepoly = type(row['geometry']) is shapely.geometry.polygon.Polygon\n",
    "    is_multipoly = type(row['geometry']) is shapely.geometry.multipolygon.MultiPolygon\n",
    "    return is_simplepoly or is_multipoly\n",
    "#     return type(row['geometry']) is shapely.geometry.polygon.Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e5b4e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matching GPX track to OSM places (uses _osm_place_download under the hood)\n",
    "points_per_batch_places = 100 # Subdivide the trail into batches of this many segments\n",
    "# n1 = 0;\n",
    "# n2 = points_per_batch_places;\n",
    "n1 = 0*points_per_batch_places;\n",
    "n2 = 5*points_per_batch_places;\n",
    "delta = 0.005\n",
    "\n",
    "# Select point subset & construct bbox\n",
    "subset = data_roads.iloc[n1:n2] # subset of points dataframe\n",
    "lat_min = subset['x0'].min() - delta\n",
    "lat_max = subset['x0'].max() + delta\n",
    "lon_min = subset['y1'].min() - delta\n",
    "lon_max = subset['y1'].max() + delta\n",
    "polygon = ox.utils_geo.bbox_to_poly(lat_max, lat_min, lon_max, lon_min) # polygon of bbox around subset\n",
    "\n",
    "# Download place data with landuse tag\n",
    "# tags = {\"landuse\": True}\n",
    "tags = {\"landuse\": ['commercial','construction','education','industrial','residential','retail','institutional','farmyard','cemetery','garages','railway','landfill','brownfield','quarry']}\n",
    "gdf = ox.geometries_from_polygon(polygon, tags)\n",
    "gdf['is_polygon'] = gdf.apply(is_polygon, axis=1)\n",
    "mask_polygon = gdf['is_polygon']==True\n",
    "gdf_subset = gdf[mask_polygon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3951641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test matching\n",
    "developed = []\n",
    "tol_d = 1.0 # Consider a segment developed if it lies closer than tol_d to a developed area\n",
    "for i, segment in subset.iterrows():\n",
    "    developed.append(False)\n",
    "    xmid = (segment['x0'] + segment['x1'])/2\n",
    "    ymid = (segment['y0'] + segment['y1'])/2\n",
    "    point = shapely.geometry.Point(ymid,xmid)\n",
    "    for j, area in gdf_subset.iterrows():\n",
    "        d = 1000*area['geometry'].distance(point)\n",
    "        if d<tol_d:\n",
    "            developed[i] = True\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f0f71308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.538\n"
     ]
    }
   ],
   "source": [
    "perc_developed = sum(developed)/len(developed)\n",
    "print(perc_developed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "26effa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map of the subset & polygons near it\n",
    "# Map setup\n",
    "chart = folium.Map(location=trail_coords[0], zoom_start=12, tiles=\"OpenStreetMap\")\n",
    "\n",
    "# Draw map matched frame\n",
    "xy = gr_plot.get_coords_from_frame(subset)\n",
    "\n",
    "# Draw areas\n",
    "for i in range(gdf_subset.shape[0]):\n",
    "    item = gdf_subset.iloc[i]\n",
    "    sim_geo = gpd.GeoSeries(item['geometry'])\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j,style_function=lambda x: {'fillColor': 'orange'})\n",
    "    folium.Popup(item['landuse']).add_to(geo_j)\n",
    "    geo_j.add_to(chart)\n",
    "    \n",
    "for point in xy:\n",
    "    newmarker = folium.CircleMarker(location=point,radius=2,color='black')\n",
    "    newmarker.add_to(chart)\n",
    "\n",
    "# Draw midpoints according to development status\n",
    "for idx, row in subset.iterrows():\n",
    "    x0 = row['x0']\n",
    "    x1 = row['x1']\n",
    "    y0 = row['y0']\n",
    "    y1 = row['y1']\n",
    "    xmid = (x0 + x1)/2\n",
    "    ymid = (y0 + y1)/2\n",
    "    midpoint = [xmid,ymid]\n",
    "#     newmarker = folium.CircleMarker(location=point,radius=2,color='black')\n",
    "#     newmarker.add_to(chart)\n",
    "    if developed[idx]:\n",
    "        newmarker = folium.CircleMarker(location=midpoint,radius=3,color='red')\n",
    "    else:\n",
    "        newmarker = folium.CircleMarker(location=midpoint,radius=3,color='green')\n",
    "    newmarker.add_to(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ffabfd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"cache/chart_development.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb551db6040>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Render the map\n",
    "filepath = \"cache/chart_development.html\"\n",
    "chart.save(filepath)\n",
    "IFrame(filepath, width=1000, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "deddb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how to process landuse into a useful statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312b617",
   "metadata": {},
   "source": [
    "## Establishing paved, traffic, and development type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482acdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e644077",
   "metadata": {},
   "source": [
    "## Saving completed data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9c24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e7f5d87",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0c488ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"data/chart_tracks.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb55fc40100>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a plot of the original and matched track\n",
    "trail_coords  = gr_mapmatch.trail_to_coords(trail)\n",
    "filepath = gr_plot.compare_tracks(trail_coords, data_roads)\n",
    "IFrame(filepath, width=1000, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911ad21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
